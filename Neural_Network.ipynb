{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "We will be using the lower resolution MINST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits # The MNIST data set is in scikit learn data set\n",
    "from sklearn.preprocessing import StandardScaler  # It is important in neural networks to scale the date\n",
    "from sklearn.model_selection import train_test_split  # The standard - train/test to prevent overfitting and choose hyperparameters\n",
    "from sklearn.metrics import accuracy_score # \n",
    "import numpy as np\n",
    "import numpy.random as r # We will randomly initialize our weights\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data\n",
    "\n",
    "After we load the data, we print the shape of the data and a pixelated digit.\n",
    "\n",
    "We also show what the features of one example looks like.\n",
    "\n",
    "The neural net will learn to estimate which digit these pixels represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the digits dataset:\n",
      "(1797, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 8 9 8]\n",
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# load all the digits (img)\n",
    "digits = load_digits()\n",
    "# load the data from the digit (img)\n",
    "\n",
    "print(\"The shape of the digits dataset:\") \n",
    "print(digits.data.shape)\n",
    "# plot the digits\n",
    "# using .gray()\n",
    "\n",
    "# and .matshow() with argument digit.images[xx]\n",
    "\n",
    "# plt.show()\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[4]) \n",
    "plt.show()\n",
    "# get the gt for this digit img\n",
    "y=digits.target\n",
    "X=digits.data\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Scale the dataset\n",
    "The training features range from 0 to 15.  To help the algorithm converge, we will scale the data to have a mean of 0 and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.33501649 -0.04308102 ... -1.14664746 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -1.09493684 ...  0.54856067 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -1.09493684 ...  1.56568555  1.6951369\n",
      "  -0.19600752]\n",
      " ...\n",
      " [ 0.         -0.33501649 -0.88456568 ... -0.12952258 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -0.67419451 ...  0.8876023  -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649  1.00877481 ...  0.8876023  -0.26113572\n",
      "  -0.19600752]]\n"
     ]
    }
   ],
   "source": [
    "# use the stander lib to scale the data\n",
    "scale=StandardScaler()\n",
    "# init the scaler \n",
    "\n",
    "# fit the data to the scaler \n",
    "X=scale.fit_transform(X)\n",
    "\n",
    "# Looking the new features after scaling\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Creating training and test datasets\n",
    "We split the data into training and test data sets. We will train the neural network with the training dataset, and evaluate our neural network with the test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 64)\n",
      "(719, 64)\n",
      "(1078,)\n",
      "(719,)\n"
     ]
    }
   ],
   "source": [
    "#Split the data into training and test set.  60% training and %40 test\n",
    "# split=int(len(X)*0.4)\n",
    "# print(len(X))\n",
    "# print(split)\n",
    "# X_train=X[:len(X)-split]\n",
    "# X_test=X[len(X)-split:]\n",
    "# y_train=y[:len(X)-split]\n",
    "# y_test=y[len(X)-split:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.6,test_size = 0.4)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "# print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Setting up the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_to_vect(y):\n",
    "#     Our target is an integer in the range [0,..,9], so we will have 10 output neuron's in our network.  \n",
    "\n",
    "#     If  y=0 we want the output neurons to have the values (1,0,0,0,0,0,0,0,0,0)\n",
    "#     If  y=1 we want the output neurons to have the values (0,1,0,0,0,0,0,0,0,0)\n",
    "#     etc\n",
    "\n",
    "#     Thus we need to change our target so it is the same as our hoped for output of the neural network. \n",
    "\n",
    "#     If y=0$we change it into the vector (1,0,0,0,0,0,0,0,0,0)\n",
    "#     If y=1 we change it into the vector (0,1,0,0,0,0,0,0,0,0)\n",
    "#     etc\n",
    "\n",
    "#     The code to covert the target vector.\n",
    "    vector=np.zeros((len(y),10), dtype=float)\n",
    "    count=0\n",
    "    for i in y:\n",
    "        idx=i\n",
    "        vector[count][idx]=1\n",
    "        count=count+1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the training and test targets to vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert digits to vectors using the func above\n",
    "y_v_train=convert_y_to_vect(y_train)\n",
    "y_v_test=convert_y_to_vect(y_test)\n",
    "# appky this to both train and test splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check to see that our code performs as we expect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 7 1 1]\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0:4])\n",
    "print(y_v_train[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Creating the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The activation function and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the sigmoid activation function:  f(z)={1}/{1+e^{-z}}\n",
    "def f(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# The deriviative of the sigmoid function is: $f'(z) = f(z)(1-f(z))$ \n",
    "def f_deriv(z):\n",
    "    fz=f(z)\n",
    "    return fz*(1-fz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "def relu_deriv(z):\n",
    "    return(z>0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "def tanh_deriv(z):\n",
    "    return 1-tanh(z)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and initialing W and b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "def setup_and_init_weights(nn_structure):\n",
    "    # The weights in W are different so that during back propagation the nodes on a level will have different gradients and thus have different update values.\n",
    "    \n",
    "    #creating a dictionary for wiehgts i.e. a set of key: value pairs\n",
    "    weights={}\n",
    "    #creating a dictionary for bias i.e. a set of key: value pairs\n",
    "    bias={}\n",
    "\n",
    "        # We want the weights to be small values, since the sigmoid is almost \"flat\" for large inputs.\n",
    "        # Next is the code that assigns each weight a number uniformly drawn from $[0.0, 1.0)$.  \n",
    "        # The code assumes that the number of neurons in each level is in the python list *nn_structure*.\n",
    "        # .random_sample return “continuous uniform” random floats in the half-open interval [0.0, 1.0). \n",
    "    weights[1]=r.random_sample((nn_structure[1], nn_structure[0]))\n",
    "    weights[2]=r.random_sample((nn_structure[2], nn_structure[1]))\n",
    "\n",
    "    bias[1]=r.random_sample((nn_structure[1],))\n",
    "    bias[2]=r.random_sample((nn_structure[2],))\n",
    "\n",
    "    return weights,bias\n",
    "\n",
    "nn_structure = [64, 30, 10]\n",
    "weights,bias=setup_and_init_weights(nn_structure)\n",
    "print(bias[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing $\\triangledown W$ and $\\triangledown b$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tri_values(nn_structure):\n",
    "    # Creating dlt_W and dlt_b to have the same size as W and b,and init the dlt_W, and dlt_b to 0\n",
    "    dlt_W={}\n",
    "    dlt_b={}\n",
    "    \n",
    "    dlt_W[1]=np.zeros((nn_structure[1], nn_structure[0]))\n",
    "    dlt_W[2]=np.zeros((nn_structure[2], nn_structure[1]))\n",
    "\n",
    "    dlt_b[1]=np.zeros((nn_structure[1],))\n",
    "    dlt_b[2]=np.zeros((nn_structure[2],))\n",
    "    \n",
    "    # use for loop to init the dlt W and dlt b\n",
    "    \n",
    "        # you can use np.zeros\n",
    "    return dlt_W,dlt_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward\n",
    "Perform a forward pass throught the network.  The function returns the values of $a$ and $z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, weights, bias):\n",
    "    # create a dictionary for holding the a values for all levels\n",
    "    a_values={}\n",
    "    # create a dictionary for holding the z values for all the layers\n",
    "    z_values={}\n",
    "    # for each layer\n",
    "    for i in range(1,len(nn_structure)): \n",
    "        a_values[1]=x\n",
    "        # z^(l+1) = W^(l)*a^(l) + b^(l)\n",
    "        z_values[i]=np.dot(weights[i],a_values[i])+bias[i]\n",
    "        # a^(l+1) = f(z^(l+1))\n",
    "        a_values[i+1]=f(z_values[i])\n",
    "\n",
    "    return a_values,z_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\delta$\n",
    "compute $\\delta^{(s_l)}$ in a function called \"calculate_out_layer_delta\",  and  computes $\\delta^{(\\ell)}$ for the hidden layers in the function called \"calculate_hidden_delta\".  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    # delta^(nl) = -(y_i - a_i^(nl)) * f'(z_i^(nl))\n",
    "    delta_nl=-(y-a_out)*f_deriv(z_out)\n",
    "    return delta_nl\n",
    "\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    delta_l=w_l.T*delta_plus_1*f_deriv(z_l)\n",
    "    return delta_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_error(y,a_values):\n",
    "    MSE=np.square(np.subtract(y,a_values)).mean()\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Back Propagation Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25):\n",
    "    # init W and b\n",
    "    weights, bias = setup_and_init_weights(nn_structure)\n",
    "    # init counter to 0\n",
    "    counter=0\n",
    "    # store the length of data\n",
    "    length=X.shape[0]\n",
    "    # init a list to store the all costs\n",
    "    cost_list=[]\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    # while the counter is less than the max iterations:\n",
    "    while counter<iter_num:\n",
    "        # print the iteration number for every 1000 iter\n",
    "        if counter % 100 == 0 :\n",
    "            print(\"iteration number\", counter)\n",
    "        \n",
    "        # init delt_W and delt_b\n",
    "        delt_W,delt_b=init_tri_values(nn_structure)\n",
    "        # init the cost to 0\n",
    "        cost=0\n",
    "        # for each data:\n",
    "        for i in range(length):\n",
    "            new_weights={}\n",
    "            data_x=X[i]\n",
    "            data_y=y[i]\n",
    "            # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            a_values,z_values=feed_forward(data_x, weights, bias)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "#         print(avg_cost)\n",
    "            for j in range(len(nn_structure)-1, 1, -1):\n",
    "                new_weights[len(nn_structure)] = calculate_out_layer_delta(data_y,a_values[len(nn_structure)],z_values[len(nn_structure)-1]) \n",
    "                new_weights[j] = calculate_hidden_delta(new_weights[j+1], weights[j], z_values[j])\n",
    "#                 print(a_values[len(nn_zaazaastructure)])\n",
    "                cost=cost+evaluate_error(data_y,a_values[len(nn_structure)])\n",
    "                delt_W[j] =delt_W[j]+ np.matmul(new_weights[j+1][:,np.newaxis],np.transpose(a_values[j][:,np.newaxis]))\n",
    "                # tri_W[l] =tri_W[l]+ np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))\n",
    "                # triW^(l) = triW^(l) + delta^(l+1) * transpose(a^(l))\n",
    "                # hit: you can use np.newaxis to increase the number of dimensions\n",
    "                        \n",
    "                # trib^(l) = trib^(l) + delta^(l+1)  \n",
    "                delt_b[j] = delt_b[j]+new_weights[j+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        \n",
    "        for k in range(1,len(nn_structure)):\n",
    "            weights[k] =weights[k]- (alpha*delt_W[k]/length)\n",
    "            bias[k] = bias[k]- (alpha*delt_b[k]/length)\n",
    "        \n",
    "        # complete the average cost (mean squared error) calculation\n",
    "        cost=cost/length\n",
    "        # append the cost to the cost list\n",
    "        cost_list.append(cost)\n",
    "        # increase the counter \n",
    "        counter=counter+1\n",
    "    return weights,bias,cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    length = X.shape[0]\n",
    "    predictions_list=[]\n",
    "    for i in range(length):\n",
    "        a_values, z_values = feed_forward(X[i], W, b)\n",
    "        prediction = np.argmax(a_values[n_layers])\n",
    "        predictions_list.append(prediction)\n",
    "    return predictions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the neural network\n",
    "\n",
    "Our code assumes the size of each layer in our network is held in a list.  The input layer will have 64 neurons (one for each pixel in our 8 by 8 pixelated digit).  Our hidden layer has 30 neurons (you can change this value).  The output layer has 10 neurons.\n",
    "\n",
    "Next we create the python list to hold the number of neurons for each level and then run the neural network code with our training data.\n",
    "\n",
    "This code will take some time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 3000 iterations\n",
      "iteration number 0\n",
      "iteration number 100\n",
      "iteration number 200\n",
      "iteration number 300\n",
      "iteration number 400\n",
      "iteration number 500\n",
      "iteration number 600\n",
      "iteration number 700\n",
      "iteration number 800\n",
      "iteration number 900\n",
      "iteration number 1000\n",
      "iteration number 1100\n",
      "iteration number 1200\n",
      "iteration number 1300\n",
      "iteration number 1400\n",
      "iteration number 1500\n",
      "iteration number 1600\n",
      "iteration number 1700\n",
      "iteration number 1800\n",
      "iteration number 1900\n",
      "iteration number 2000\n",
      "iteration number 2100\n",
      "iteration number 2200\n",
      "iteration number 2300\n",
      "iteration number 2400\n",
      "iteration number 2500\n",
      "iteration number 2600\n",
      "iteration number 2700\n",
      "iteration number 2800\n",
      "iteration number 2900\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [64, 30, 10]\n",
    "\n",
    "# train the NN with the nn_structure and 3000 iterations\n",
    "W, b, costList = train_nn(nn_structure, X_train,y_v_train, 3000,0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the learning curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(costList)\n",
    "# x label: \"Iteration number\"\n",
    "plt.xlabel(\"Iteration number\")\n",
    "# y label: \"Average J\"\n",
    "plt.ylabel(\"Average J\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Assessing accuracy\n",
    "Next we determine what percentage the neural network correctly predicted the handwritten digit correctly on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 67.17663421418636%\n"
     ]
    }
   ],
   "source": [
    "# get the prediction accuracy and print\n",
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
